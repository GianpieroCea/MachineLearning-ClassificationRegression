{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#assignmnet 1 tna regression\n",
    "\n",
    "#data\n",
    "#loads the raw dataframes\n",
    "probeA = pd.read_csv(\"C:\\Users\\pc\\Python_Projects\\MachineLearningWARWICK\\data\\probeData\\probeA.csv\",header=0)\n",
    "probeB = pd.read_csv(\"C:\\Users\\pc\\Python_Projects\\MachineLearningWARWICK\\data\\probeData\\probeB.csv\",header=0)\n",
    "\n",
    "# some values are \"swapped\" betweens columns, so we need to reorder:\n",
    "def reorder(df):\n",
    "    # From the initial data exploration it was clear that ther was some corruption in the form of a pemutation \n",
    "    #for each of the 4 proteins. This code reorders it.\n",
    "    copydf=df.copy()\n",
    "    for letter in [\"c\",\"m\",\"n\",\"p\"]:\n",
    "        old_c = copydf[[letter+\"1\",letter+\"2\",letter+\"3\"]]\n",
    "        c = old_c.values\n",
    "        c.sort(axis=1)\n",
    "        c_df = pd.DataFrame(c,columns=old_c.columns)\n",
    "        copydf[old_c.columns] = c_df\n",
    "    return copydf\n",
    "\n",
    "#scale data in standard way\n",
    "def scale_data(dataFrame):\n",
    "    df = dataFrame.copy()\n",
    "\n",
    "    for var in df:\n",
    "        mean = df[var].mean()\n",
    "        std = df[var].std()\n",
    "        assert(std != 0)\n",
    "        df[var] = (df[var]-mean)/std\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning up and preprocessing\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "#reorder data\n",
    "probeA = reorder(probeA)\n",
    "probeB = reorder(probeB)\n",
    "\n",
    "#stores everything apart tna and class column\n",
    "probeA_data =probeA.drop('tna',1).drop('class',1)\n",
    "probeA_data_std = scale_data(probeA_data)\n",
    "\n",
    "probeB_data =probeB\n",
    "probeB_data_std = scale_data(probeB_data)\n",
    "\n",
    "tna_target = probeA['tna']\n",
    "class_target = probeA['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature expansion\n",
    "def polynomial_feature_ord(X,n):\n",
    "    poly = pp.PolynomialFeatures(n)\n",
    "    out = poly.fit_transform(X)\n",
    "    feature_names = poly.get_feature_names(X.columns)\n",
    "    \n",
    "    X_new = pd.DataFrame(out,columns =feature_names)\n",
    "    return X_new\n",
    "\n",
    "data_ord_2 = polynomial_feature_ord(probeA_data_std,2)\n",
    "data_ord_3 = polynomial_feature_ord(probeA_data_std,3)\n",
    "data_ord_4 = polynomial_feature_ord(probeA_data_std,4)\n",
    "\n",
    "data_tna = probeA_data_std.copy()\n",
    "data_tna['tna'] = tna_target\n",
    "\n",
    "data_ord_2_tna = polynomial_feature_ord(data_tna,2).drop('1',1)\n",
    "\n",
    "data_ord_3_tna = polynomial_feature_ord(data_tna,3).drop('1',1)\n",
    "\n",
    "data_ord_4_tna = polynomial_feature_ord(data_tna,4).drop('1',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
      "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
      "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
      "    verbose=False)\n",
      "1.34593265907\n",
      "1.51478939396\n",
      "1.40785401555\n",
      "0.864686058977\n",
      "1.02516447559\n",
      "0.734130692471\n",
      "1.16452013068\n",
      "1.68023066435\n",
      "1.93813960025\n",
      "1.26495910339\n",
      "0.854167209224\n",
      "1.12594120112\n",
      "1.37702918413\n",
      "0.819238594163\n",
      "1.07178417726\n",
      "1.97247855278\n",
      "1.21649645637\n",
      "1.10090296206\n",
      "0.804556420167\n",
      "1.21304641679\n",
      "2.18167171119\n",
      "0.688206793379\n",
      "2.17049983408\n",
      "1.08893280754\n",
      "1.01061998284\n",
      "1.66120422229\n",
      "0.997378815215\n",
      "1.09606738893\n",
      "1.73506066397\n",
      "0.92670685251\n",
      "0.52977958024\n",
      "1.54997037143\n",
      "1.06766508693\n",
      "0.872942218827\n",
      "0.877124643788\n",
      "0.858365421937\n",
      "1.48617314229\n",
      "1.03018085464\n",
      "1.88258730267\n",
      "1.33085211655\n",
      "1.13985376091\n",
      "1.01935747956\n",
      "0.707902667755\n",
      "1.136663048\n",
      "1.25359609595\n",
      "1.30834546035\n",
      "0.691684838725\n",
      "1.32622175777\n",
      "1.23553911226\n",
      "1.15281088939\n",
      "error for model: 1.21020025781\n",
      "0.0206196132271\n",
      "RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,\n",
      "    normalize=False, scoring=None, store_cv_values=False)\n",
      "1.33002904924\n",
      "1.53017474582\n",
      "1.41019524051\n",
      "0.874227009388\n",
      "1.05149312018\n",
      "0.734426828459\n",
      "1.09006884425\n",
      "1.67480024304\n",
      "2.00178685012\n",
      "1.24123060296\n",
      "0.851028022693\n",
      "1.10725088731\n",
      "1.37067421828\n",
      "0.821220274894\n",
      "1.06278216424\n",
      "1.97838439179\n",
      "1.20182576219\n",
      "1.10845551099\n",
      "0.812304520239\n",
      "1.22851742986\n",
      "2.23460449001\n",
      "0.690892275702\n",
      "2.11908576378\n",
      "1.10205482014\n",
      "1.01065028322\n",
      "1.65998944874\n",
      "1.01843693565\n",
      "1.09822911547\n",
      "1.75695513802\n",
      "0.920381778025\n",
      "0.524216197702\n",
      "1.52713537211\n",
      "1.07401989011\n",
      "0.873756596183\n",
      "0.846678968304\n",
      "0.854887098953\n",
      "1.47816095692\n",
      "1.03608505911\n",
      "1.87619173606\n",
      "1.34566735781\n",
      "1.13940804199\n",
      "0.993898264069\n",
      "0.714534230327\n",
      "1.10386333207\n",
      "1.25278267445\n",
      "1.31152141092\n",
      "0.701083777935\n",
      "1.3232753721\n",
      "1.24137163353\n",
      "1.14908614984\n",
      "error for model: 1.20919559771\n",
      "10.0\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "1.3237590532\n",
      "1.53285372489\n",
      "1.4153329076\n",
      "0.877364012134\n",
      "1.0552736595\n",
      "0.735422260452\n",
      "1.0745030294\n",
      "1.67238494249\n",
      "2.02593544316\n",
      "1.24397009145\n",
      "0.851620689604\n",
      "1.10771935893\n",
      "1.36729735435\n",
      "0.816382333569\n",
      "1.06180229233\n",
      "1.98642961639\n",
      "1.19112933557\n",
      "1.10952042223\n",
      "0.816414410612\n",
      "1.2282136641\n",
      "2.24134014383\n",
      "0.689863336794\n",
      "2.11006212792\n",
      "1.10886355803\n",
      "1.01043758491\n",
      "1.66523069798\n",
      "1.02741451544\n",
      "1.09706089525\n",
      "1.75585527153\n",
      "0.9249931248\n",
      "0.528539818212\n",
      "1.51633385817\n",
      "1.07595484111\n",
      "0.879207030665\n",
      "0.845781215539\n",
      "0.860532535117\n",
      "1.48324490988\n",
      "1.0400307088\n",
      "1.87745844023\n",
      "1.33585005628\n",
      "1.14035216941\n",
      "0.983758104636\n",
      "0.717747998491\n",
      "1.09372870854\n",
      "1.23859425628\n",
      "1.31102212921\n",
      "0.702803793593\n",
      "1.32079371098\n",
      "1.22614137431\n",
      "1.14061564601\n",
      "error for model: 1.20885882328\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'alpha_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1e014e43eb87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error for model: '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_err\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'alpha_'"
     ]
    }
   ],
   "source": [
    "# regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#here we arbitrary feature selected and we imporved generally!\n",
    "#ridge ends up doing better!\n",
    "\n",
    "features = ['c3' ,'c1^2', 'm1 n3' ,'m2^2', 'n1 p1', 'n3^2', 'p1^2']\n",
    "data = data_ord_2[features]\n",
    "\n",
    "\n",
    "models = [LassoCV(),RidgeCV(),LinearRegression()]\n",
    "fitted_models = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    m = model\n",
    "    kf = cv.KFold(1000,50)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "    print('error for model: '+str(sum_err/50))\n",
    "    print(model.alpha_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#exploration to find important features in order 2 expansion\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#found before: 0.0914285714286,0.0914285714286\n",
    "#but with LassoCV: 0.0629693194436 , 0.0629694\n",
    "\n",
    "\n",
    "for alpha in np.linspace( 0.0629693194436 , 0.0629694):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Lasso(alpha)\n",
    "    kf = cv.KFold(1000,50)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/50))\n",
    "    print('###########################')\n",
    "    \n",
    "    \n",
    "    #result: looks like \n",
    "    #['c3' 'c1^2' 'm1 n3' 'm2^2' 'n1 p1' 'n3^2' 'p1^2'] are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we try to use the feature selected above to see if we have any improvements\n",
    "features = ['c3' ,'c1^2', 'm1 n3' ,'m2^2', 'n1 p1', 'n3^2', 'p1^2']\n",
    "\n",
    "data = data_ord_2[features]\n",
    "print(data)\n",
    "for alpha in np.linspace( 0.0629693194436 , 0.0629694):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Lasso(alpha)\n",
    "    kf = cv.KFold(1000,50)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        #print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/50))\n",
    "    print('###########################')\n",
    "    \n",
    "    \n",
    "    #With the feature selected we actually got better values!\n",
    "    #error with order 2 1.21852975368\n",
    "    #changing order didn't seem to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we now do similar things to above but for ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "features = ['c1', 'c2', 'c3', 'c1 c2', 'c1 c3', 'c1 n3' ,'c1 p1', 'c1 p2' ,'c3 n2', 'c3 n3','m1 n3' ,'n1 p1' ,'n2 p1' ,'n2 p2' ,'p1^2' ,'p2 p3']\n",
    "#tried this aswell but works worse\n",
    "features2 = ['c3' ,'c1^2', 'm1 n3' ,'m2^2', 'n1 p1', 'n3^2', 'p1^2']\n",
    "data = data_ord_2[features]\n",
    "\n",
    "\n",
    "for alpha in np.linspace(6.49747554165,50):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Ridge(alpha)\n",
    "    kf = cv.KFold(1000,100)\n",
    "    sum_err = 0\n",
    "    det_score = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        #print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        det_score += m.score(X_test,y_test)\n",
    "        #print(det_score)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/100)+'  with alpha:'+str(alpha)+'and order 2')\n",
    "    print('determination score'+str(det_score/float(100)))\n",
    "    print('###########################')\n",
    "    \n",
    "    #0.0408163265306,0.0816326530612\n",
    "    #0.0483132028321,0.0499791753436\n",
    "    #result: looks like \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## from sklearn.linear_model import RidgeCV\n",
    "rid = RidgeCV()\n",
    "rid.fit(data_ord_3,tna_target)\n",
    "rid.alpha_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems Ridge regression does far worse than Lasso where we obtain some good mean squared error.\n",
    "With this we get 1.21852975368 with poly order 2 and automatic cross val over the alpha and feature selection to ['c3' 'c1^2' 'm1 n3' 'm2^2' 'n1 p1' 'n3^2' 'p1^2']. \n",
    "We get similar results with manual search between 0.0483132028321,0.0499791753436 without having to do feature selection.\n",
    "In both Rifge and Lasso it seemed like increasing the order didn't bring any improvement to the mean squared error.\n",
    "With an arbitrary threshold it looke like lasso said these are some important features:\n",
    "['c3' 'c1^2' 'm1 n3' 'm2^2' 'n1 p1' 'n3^2' 'p1^2']\n",
    "Ridge bestC alpha cv seem to be 10 for all order.\n",
    "Also ridge important features seem to be the following:\n",
    "\n",
    "CHANGE! Actually Ridge is better: 1.20311622873  obtained with order 2 data feature selected with ['c1', 'c2', 'c3', 'c1 c2', 'c1 c3', 'c1 n3' ,'c1 p1', 'c1 p2' ,'c3 n2', 'c3 n3','m1 n3' ,'n1 p1' ,'n2 p1' ,'n2 p2' ,'p1^2' ,'p2 p3'].\n",
    "Even with the regularisation 10 found by RidgeCV we get lower than lasso.\n",
    "UPDATE! Using Ridge as before (order 2, fatures ['c1', 'c2', 'c3', 'c1 c2', 'c1 c3', 'c1 n3' ,'c1 p1', 'c1 p2' ,'c3 n2', 'c3 n3','m1 n3' ,'n1 p1' ,'n2 p1' ,'n2 p2' ,'p1^2' ,'p2 p3']) but with alpha between 6.49625156185,6.49750104123 we obtain 1.20292815273 !!\n",
    "WEIRDLY ENOUGH, the Determination Score determination score0.765028763036 is maximised aroun 10.5 or something but then the error increases...\n",
    "\n",
    "BUT: So far the best regression with both Score determination and means squared error haas been given by LassoCV which finds and alpha value of 0.0518035145118 with ORDER 3 and not even any feature selection!! The MSE and R2 score are:\n",
    "1.10840298353\n",
    "0.87571087707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uses f_classif to SelectKBest\n",
    "#experiment to see if it matches with Lasso feature select above\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_regression, k=7)\n",
    "selector.fit(data_ord_2, tna_target)\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "#scores = -np.log10(selector.pvalues_)\n",
    "print zip(selector.get_support(),data_ord_3.columns)\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(data_ord_3.columns)), scores)\n",
    "plt.xticks(range(len(data_ord_3.columns)), data_ord_3.columns, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# with order 2, and f_regression\n",
    "# (True, 'p1^2'), (True, 'p1 p2'), (True, 'p1 p3'), (True, 'p2^2'), (True, 'p2 p3'), (True, 'p3^2')\n",
    "#not very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.32133275097\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call `fit` before `feature_importances_`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-136969820b2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdepth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#best_depth =7 for order 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#0.658422546097\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pc\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \"\"\"\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             raise NotFittedError(\"Estimator not fitted, call `fit` before\"\n\u001b[0m\u001b[0;32m    499\u001b[0m                                  \" `feature_importances_`.\")\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator not fitted, call `fit` before `feature_importances_`."
     ]
    }
   ],
   "source": [
    "#decision tree regression\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "features=['c1', 'c2', 'c3', 'c1 c2', 'c1 c3', 'c1 n3' ,'c1 p1', 'c1 p2' ,'c3 n2', 'c3 n3','m1 n3' ,'n1 p1' ,'n2 p1' ,'n2 p2' ,'p1^2' ,'p2 p3']\n",
    "\n",
    "features2 = [u'c3', u'm1 n3', u'm3 n3', u'n1 n3', u'p1^2']\n",
    "data = data_ord_2[features2]\n",
    "depth = []\n",
    "for i in range(1,60):\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=i)\n",
    "    scores = cross_val_score(estimator=clf, X=data, y=tna_target, cv=10)\n",
    "    print(str(i)+' '+str(scores.mean()))\n",
    "    depth.append((i,scores.mean()))\n",
    "    \n",
    "#best_depth =7 for order 2\n",
    "#0.658422546097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'c3', u'c3 m3', u'm1 n3', u'm3 n3', u'p1^2'], dtype='object')\n",
      "1.08730720125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x108dcbe0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4zJQRbuxhiTg/4/jYxGtGx1SnEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d46d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#decision tree regression: best features\n",
    "from sklearn import tree\n",
    "\n",
    "def cutoff_importances(importances,threshold,names):\n",
    "    compare = importances > threshold\n",
    "    return names[compare == True]\n",
    "\n",
    "max_depth =7\n",
    "clf = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "clf.fit(data_ord_2,tna_target)\n",
    "#arbitrary threshold explore\n",
    "print cutoff_importances(clf.feature_importances_,0.5e-2,data_ord_2.columns)\n",
    "\n",
    "#mean square error of this model\n",
    "print mean_squared_error(clf.predict(data_ord_2),tna_target)\n",
    "\n",
    "#Index([u'c3', u'm1 n3', u'm3 n3', u'n1 n3', u'p1^2'], dtype='object')\n",
    "#so similar values\n",
    "\n",
    "\n",
    "sns.distplot((predict-predict.mean())/(predict.std()))\n",
    "sns.distplot((tna_target-tna_target.mean())/(tna_target.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We redo decision tree regressor this time with cross validation done mannually\n",
    "features=['c1', 'c2', 'c3', 'c1 c2', 'c1 c3', 'c1 n3' ,'c1 p1', 'c1 p2' ,'c3 n2', 'c3 n3','m1 n3' ,'n1 p1' ,'n2 p1' ,'n2 p2' ,'p1^2' ,'p2 p3']\n",
    "features2 = [u'c3', u'm1 n3', u'm3 n3', u'n1 n3', u'p1^2']\n",
    "features3 = ['c3','p3','m1 n3','m2 n1','n3^2' ,'p1^2' ,'c1 c3 m1' ,'c1 m2 m3', 'c1 m2 n2','c1 m3 p3' ,'c2 m1 m3' ,'c2 n2 p2' ,'c3 m3 n2' ,'c3 n1 p2' ,'c3 p1^2','n2 p2 p3']\n",
    "data = data_ord_2[features]\n",
    "\n",
    "\n",
    "for max_depth in range(1,100):\n",
    "\n",
    "    m = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    kf = cv.KFold(1000,10)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += m.score(X_test,y_test)\n",
    "        #print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/10)+'with depth: '+str(max_depth))\n",
    "    print('###########################')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#exploration to find important features in order 2 expansion\n",
    "data = data_ord_2\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for alpha in np.linspace(0.0489795918367,0.0514285714286):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Lasso(alpha)\n",
    "    kf = cv.KFold(1000,10)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        #print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/10))\n",
    "    print('###########################')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_ord_3\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for alpha in np.linspace(0.051,0.052):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Lasso(alpha)\n",
    "    kf = cv.KFold(1000,10)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        #print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/10))\n",
    "    print('###########################')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot((tna_target-tna_target.mean())/(tna_target.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew \n",
    "\n",
    "sns.distplot((tna_target-tna_target.mean())/(tna_target.std()))\n",
    "#print skew(tna_target)\n",
    "#interesting: the data is skewed on the left...what does it mean?\n",
    "\n",
    "m = LassoCV()\n",
    "\n",
    "m.fit(data_ord_3,tna_target)\n",
    "print(m.alpha_)\n",
    "predict = m.predict(data_ord_3)\n",
    "#print skew(predict)\n",
    "print(mean_squared_error(predict,tna_target))\n",
    "print(m.score(data_ord_3,tna_target))\n",
    "print(m.coef_)\n",
    "print(m.get_support())\n",
    "sns.distplot((predict-predict.mean())/(predict.std()))\n",
    "\n",
    "#LassoCV finds a alpha =0.0518035145118\n",
    "#for order 3 expan..this much better then order 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try the things from before with cross validation:\n",
    "features =[u'c3', u'm1 n3', u'm3 n3', u'n1 n3', u'p1^2']\n",
    "data = data_ord_2[features]\n",
    "kf = cv.KFold(1000,10)\n",
    "\n",
    "#for alpha in np.linspace(0.0043, 0.0046):\n",
    "#print(alpha)\n",
    "sum_score = 0\n",
    "sum_err = 0\n",
    "for train,test in kf:\n",
    "    print('start fold')\n",
    "    X_train = data.iloc[train].copy()\n",
    "    y_train  = tna_target.iloc[train].copy()\n",
    "    X_test = data.iloc[test].copy()\n",
    "    y_test  = tna_target.iloc[test].copy()\n",
    "    m = Lasso(alpha=0.0043)\n",
    "    #m = LassoCV()\n",
    "    fitted = m.fit(X_train,y_train)\n",
    "\n",
    "    feature_names = np.array(X_train.columns)\n",
    "    model = SelectFromModel(m, prefit=True)\n",
    "    selected_features = feature_names[model.get_support()]\n",
    "    #print selected_features\n",
    "    predict = m.predict(X_test)\n",
    "    sum_err += mean_squared_error(predict,y_test)\n",
    "    sum_score += fitted.score(X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('score for model: '+str(sum_score/10))\n",
    "print('error for model: '+str(sum_err/10))\n",
    "print('###########################')\n",
    "#alpha = 0.0043\n",
    "#score for model: 0.851696864765\n",
    "#error for model: 1.21039657058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had forgot about cross validation but it still looks like we 10 fold we get 0.840584952765 det score using order 3 and no feature selection using lassoCV.\n",
    "Order 4 becomes significantly worse.\n",
    "ACTUALLY! Order 2 is better than order 3, no feature selection with LassoCV.0.849724840956\n",
    "CHANGE! Maybe using LassoCV in each fold was over fitting? To be sure, we can pick just the alpha 0.0535918367347 that gives us\n",
    "score for model: 0.85003332207\n",
    "error for model: 1.22344836438\n",
    "\n",
    "Even better with features =[u'c3', u'm1 n3', u'm3 n3', u'n1 n3', u'p1^2'] and alpha = 0.0043,score for model: 0.851696864765,error for model: 1.21039657058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_ord_3\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "for alpha in np.linspace(1.0,20):\n",
    "    print(alpha)\n",
    "    print('#######################')\n",
    "    m = Ridge(alpha)\n",
    "    kf = cv.KFold(1000,10)\n",
    "    sum_err = 0\n",
    "    for train,test in kf:\n",
    "\n",
    "        \n",
    "        X_train = data.iloc[train].copy()\n",
    "        y_train  = tna_target.iloc[train].copy()\n",
    "        X_test = data.iloc[test].copy()\n",
    "        y_test  = tna_target.iloc[test].copy()\n",
    "        \n",
    "        fitted = m.fit(X_train,y_train)\n",
    "        feature_names = np.array(X_train.columns)\n",
    "        model = SelectFromModel(m, prefit=True)\n",
    "        selected_features = feature_names[model.get_support()]\n",
    "        #print selected_features\n",
    "        predict = m.predict(X_test)\n",
    "        sum_err += mean_squared_error(predict,y_test)\n",
    "        print(mean_squared_error(predict,y_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    print('error for model: '+str(sum_err/10))\n",
    "    print('###########################')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew \n",
    "\n",
    "sns.distplot((tna_target-tna_target.mean())/(tna_target.std()))\n",
    "print skew(tna_target)\n",
    "#interesting: the data is skewed on the left...what does it mean?\n",
    "\n",
    "m = RidgeCV()\n",
    "\n",
    "m.fit(data_ord_3,tna_target)\n",
    "print(m.alpha_)\n",
    "predict = m.predict(data_ord_3)\n",
    "print skew(predict)\n",
    "print(mean_squared_error(predict,tna_target))\n",
    "sns.distplot((predict-predict.mean())/(predict.std()))\n",
    "\n",
    "#RidgeCV finds a alpha =10\n",
    "#for order 3 expan..this much better then order 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(probeA_data_std['p2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probeA_data_std_class = probeA_data_std\n",
    "probeA_data_std_class['class'] = class_target\n",
    "probeA_data_std_class.head()\n",
    "sns.pairplot(probeA_data_std_class,hue='class',kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(probeB_data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can observe that the data from probA and probB seem to follow the same general pattern so we can assume that probeB will be the test set of the train set probA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #classification\n",
    "    \n",
    "#first we do it without trying to use tna\n",
    "#decision tree classifier\n",
    "\n",
    "#decision tree regression\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "depth = []\n",
    "for i in range(1,100):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i)\n",
    "    scores = cross_val_score(estimator=clf, X=data_ord_4, y=class_target, cv=10)\n",
    "    print(scores.mean())\n",
    "    depth.append((i,scores.mean()))\n",
    "#best_depth = 3 for data ord 2\n",
    "#0.710044\n",
    "#best_depth = 3 for data ord 3 aswell\n",
    "#0.70303460346\n",
    "#data ord 4 seems inferior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decsion tree importances\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(data_ord_2,class_target)\n",
    "importances = clf.feature_importances_\n",
    "#arbitrary thre\n",
    "print cutoff_importances(importances,0.017,data_ord_2.columns)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=data_ord_2.columns,  \n",
    "                         class_names=['0','1'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "#print the tree\n",
    "print(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use website:\n",
    "https://dreampuf.github.io/GraphvizOnline/\n",
    "To render the DOT file above and get a graphical representation of the decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = pd.read_csv(\"C:\\Users\\pc\\Python_Projects\\MachineLearningWARWICK\\data\\probeData\\probeB.csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we do it using tna (the actual values!)\n",
    "#decision tree classifier\n",
    "\n",
    "#decision tree regression\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "depth = []\n",
    "for i in range(1,100):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i)\n",
    "    scores = cross_val_score(estimator=clf, X=data_ord_2_tna, y=class_target, cv=10)\n",
    "    print(scores.mean())\n",
    "    depth.append((i,scores.mean()))\n",
    "'''   \n",
    "0.78600550055\n",
    "0.776065906591\n",
    "0.83001740174\n",
    "0.82803720372\n",
    "0.83003680368\n",
    "0.82203620362\n",
    "0.822066506651\n",
    "0.81901640164\n",
    "0.805055405541\n",
    "0.79904570457\n",
    "0.80703580358\n",
    "0.808075807581\n",
    "0.80000590059\n",
    "0.8010360036\n",
    "0.805085508551\n",
    "0.801065906591\n",
    "0.80704610461\n",
    "0.808065706571\n",
    "0.801055605561\n",
    "0.821076207621\n",
    "0.793056205621\n",
    "0.81002630263\n",
    "0.810056205621\n",
    "0.8050360036\n",
    "0.81101640164\n",
    "0.804065706571\n",
    "0.81001620162\n",
    "0.8089959996\n",
    "0.81603620362\n",
    "0.82000590059\n",
    "0.806065306531\n",
    "0.80100570057\n",
    "0.81697639764\n",
    "0.807065906591\n",
    "0.80702610261\n",
    "0.81800650065\n",
    "0.80800590059\n",
    "0.80198649865\n",
    "0.80704590459\n",
    "0.80004590459\n",
    "0.80600570057\n",
    "0.81201620162\n",
    "0.806055805581\n",
    "0.802055805581\n",
    "0.80001580158\n",
    "0.80403580358\n",
    "0.809056005601\n",
    "0.81604610461\n",
    "0.80104610461\n",
    "0.8120160016\n",
    "0.80602570257\n",
    "0.81703620362\n",
    "0.805065706571\n",
    "0.80804570457\n",
    "0.80202590259\n",
    "0.80802570257\n",
    "0.81102630263\n",
    "0.81501620162\n",
    "0.81096629663\n",
    "0.81202630263\n",
    "0.79801560156\n",
    "0.81000610061\n",
    "0.801075807581\n",
    "0.79604570457\n",
    "0.80501580158\n",
    "0.799056005601\n",
    "0.804066106611\n",
    "0.81501620162\n",
    "0.80500630063\n",
    "0.804055805581\n",
    "0.798056005601\n",
    "0.8050360036\n",
    "0.80902630263\n",
    "0.7960160016\n",
    "0.79602590259\n",
    "0.798075607561\n",
    "0.80104550455\n",
    "0.81202610261\n",
    "0.802065306531\n",
    "0.80704610461\n",
    "0.8070160016\n",
    "0.81903640364\n",
    "0.80702570257\n",
    "0.803055805581\n",
    "0.80403620362\n",
    "0.8000160016\n",
    "0.81204610461\n",
    "0.80402610261\n",
    "0.79603580358\n",
    "0.802065706571\n",
    "0.804055805581\n",
    "0.80604590459\n",
    "0.80702590259\n",
    "0.80304550455\n",
    "0.80503560356\n",
    "0.81102590259\n",
    "0.801055205521\n",
    "0.81400630063\n",
    "0.79703580358\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop ones\n",
    "\n",
    "data_ord_2_bias = data_ord_2.copy()\n",
    "data_ord_2_bias = data_ord_2.drop('1',1)\n",
    "data_ord_3_bias = data_ord_3.copy()\n",
    "data_ord_3_bias = data_ord_3.drop('1',1)\n",
    "data_ord_4_bias = data_ord_4.copy()\n",
    "data_ord_4_bias = data_ord_4.drop('1',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "1) use best regression method so far, get predictions for TNA and then try classifying 'class' remebering to feature expand with the predicted tna\n",
    "2) use k-nn with weighted malanhobis distance to classify . Without tna and with as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kNN for classification using Mahlanobis without using tna\n",
    "# use\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = data_ord_2_tna\n",
    "\n",
    "\n",
    "kf = cv.KFold(1000,10)\n",
    "sum_err= 0\n",
    "for train,test in kf:\n",
    "\n",
    "\n",
    "    X_train = data.iloc[train].copy()\n",
    "    y_train  = class_target.iloc[train].copy()\n",
    "    X_test = data.iloc[test].copy()\n",
    "    y_test  = class_target.iloc[test].copy()\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=50,metric='mahalanobis',metric_params={'V': np.cov(data.transpose())},weights='distance')\n",
    "\n",
    "    predict = knn.predict(X_test)\n",
    "    sum_err += accuracy_score(predict,y_test)\n",
    "    print(accuracy_score(predict,y_test))\n",
    "print('average accuracy: '+str(sum_err/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ord_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the problem above: how to use poly feature with malanobi knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda2]",
   "language": "python",
   "name": "Python [Anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
